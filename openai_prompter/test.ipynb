{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d115325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f25f004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt engineering\n",
      "================== \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"openai_prompter/Prompter.txt\") as f:     ## great way to read files with open(\"txt\") as f:   text = f.read()\n",
    "    text = f.read()\n",
    "print(text[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455a6b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "Template = \"\"\"\n",
    "{text}\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Based on the above help me build a good prompt Template!\n",
    "This template should be a python f-string. It can take in any number of variables depending on my objective.\n",
    "Return you answer in the folowing format:\n",
    "```prompt\n",
    "...\n",
    "```\n",
    "This is my objective:\n",
    "{objective}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f10050bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(Template)                                                 # translates the text template to a prompt \n",
    "prompt = prompt.partial(text = text)                                                            # since text is something that doesnt change we can add it here instead of chain.invoke({\"text\":text,...}) so now we can remove it form the invoke\n",
    "chain  = prompt | ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0) | StrOutputParser()     # usually model =ChatOpenAI(.....) then ...| model |..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13c1e403",
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = \"answer a question based on provided, and ONLY on that context.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e885c5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on your objective to answer a question based on provided context and only on that context, here is a Python f-string prompt template that you can use. This template allows you to insert the context and the question as variables, ensuring that the model\\'s response is based solely on the given context.\\n\\n```python\\ncontext = \"\"\"Your context goes here. This should be the information that the model will use to answer the question.\"\"\"\\nquestion = \"Your question goes here?\"\\n\\nprompt = f\"\"\"\\n# Context\\nThe following information is provided for reference:\\n\\n{context}\\n\\n# Question\\n{question}\\n\\n# Answer\\nBased on the provided context, the answer is:\\n\"\"\"\\n\\nprint(prompt)\\n```\\n\\nWhen you use this template, replace the placeholder text for `context` and `question` with the actual context and question you want to ask. The model will then generate an answer based on the context provided.\\n\\nHere\\'s how you would use the template in practice:\\n\\n```python\\ncontext = \"\"\"Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, \\'and what is the use of a book,\\' thought Alice \\'without pictures or conversation?\\'\"\"\"\\nquestion = \"Why was Alice getting tired?\"\\n\\nprompt = f\"\"\"\\n# Context\\nThe following information is provided for reference:\\n\\n{context}\\n\\n# Question\\n{question}\\n\\n# Answer\\nBased on the provided context, the answer is:\\n\"\"\"\\n\\nprint(prompt)\\n```\\n\\nThis will output a prompt that is ready to be sent to the model for generating an answer.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"objective\":objective})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb32f70f",
   "metadata": {},
   "source": [
    "## For cleaner outputs we can use STREAM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92f5edcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on your objective to answer a question based on provided context and only on that context, here's a prompt template in Python f-string format:\n",
      "\n",
      "```python\n",
      "prompt = f\"\"\"\n",
      "# Context\n",
      "{context}\n",
      "\n",
      "# Instructions\n",
      "You are an assistant with access to the information provided above. Your task is to answer the question based solely on the context given. Do not use external knowledge or make assumptions beyond the provided context. Provide a clear and concise answer.\n",
      "\n",
      "# Question\n",
      "{question}\n",
      "\n",
      "# Answer\n",
      "\"\"\"\n",
      "```\n",
      "\n",
      "To use this template, you would replace `{context}` with the actual context text and `{question}` with the question you want the model to answer. Here's an example of how you might use the template in a Python script:\n",
      "\n",
      "```python\n",
      "context = \"The quick brown fox jumps over the lazy dog. This sentence contains every letter of the alphabet, making it a pangram.\"\n",
      "question = \"What is special about the sentence mentioned in the context?\"\n",
      "\n",
      "# Using the prompt template\n",
      "final_prompt = prompt.format(context=context, question=question)\n",
      "\n",
      "# Now you can send `final_prompt` to the model for generation\n",
      "# For example:\n",
      "# response = openai.Completion.create(engine=\"gpt-5\", prompt=final_prompt, max_tokens=150)\n",
      "```\n",
      "\n",
      "Remember to replace `\"gpt-5\"` with the actual model you're using and adjust `max_tokens` as needed. The `response` will contain the model's answer based on the provided context."
     ]
    }
   ],
   "source": [
    "for token in chain.stream({\"objective\":objective}):\n",
    "    print(token, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
